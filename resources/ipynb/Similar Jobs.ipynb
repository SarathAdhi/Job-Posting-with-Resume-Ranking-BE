{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser\n",
    "import pandas as pd\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = set(stopwords.words('english'))\n",
    "\n",
    "df = pd.read_csv('../../others/job_final.csv')\n",
    "df['test'] = df['Job_Description'].apply(\n",
    "  lambda x: ' '.join(\n",
    "    [word for word in str(x).split() if len(word) > 2 and word not in (stopw)]\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanResume(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    \n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=3):\n",
    "    string = fix_text(string)  # fix text\n",
    "    # remove non ascii chars\n",
    "    string = string.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    string = string.lower()\n",
    "    chars_to_remove = [\")\", \"(\", \".\", \"|\", \"[\", \"]\", \"{\", \"}\", \"'\"]\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    string = re.sub(rx, '', string)\n",
    "    string = string.replace('&', 'and')\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('-', ' ')\n",
    "    string = string.title()  \n",
    "    # normalise case - capital at start of each word\n",
    "    # get rid of multiple spaces and replace with a single\n",
    "    string = re.sub(' +', ' ', string).strip()\n",
    "    string = ' ' + string + ' '  # pad names for ngrams...\n",
    "    string = re.sub(r'[,-./]|\\sBD', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    \n",
    "    return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearestN(vectorizer, nbrs, query):\n",
    "    queryTFIDF_ = vectorizer.transform(query)\n",
    "    distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cocsa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sql Html5 Javascript Css Video Python Website Php Ui Ux Computer science R C++ C Ai Editing P Programming Computer Science']\n",
      "Vecorizing completed...\n",
      "   index                         Position                 Company      Location\n",
      "0  1886   Lead Frontend Developer          Meaww                   Bengaluru   \n",
      "1  532    Web Developer                    Netrovert Software       – Bengaluru\n",
      "2  601    Web Designer                     VR CAREERZ               – Chennai  \n",
      "3  1883   Full Stack Developer             Netrovert Software      Bengaluru   \n",
      "4  1656   Front End Developer              Netrovert Software      Bengaluru   \n",
      "5  1855   Senior UI Developer              Riversand Technologies  Bengaluru   \n",
      "6  1401   Frontend Developer               Bengaluru               Bengaluru   \n",
      "7  1384   Frontend Developer               Play Games24x7          Bengaluru   \n",
      "8  1402   Front End Developer              Bengaluru               Bengaluru   \n",
      "9  900    Data Scientist / Scala Engineer  IQLECT                  Bengaluru   \n"
     ]
    }
   ],
   "source": [
    "data = ResumeParser(\"../resumes/Sarath_Resume.pdf\").get_extracted_data()\n",
    "\n",
    "resume = data['skills']\n",
    "\n",
    "skills = []\n",
    "skills.append(' '.join(word for word in resume))\n",
    "org_name_clean = skills\n",
    "org_name_clean[0] += \" \" + data['name']\n",
    "\n",
    "print(org_name_clean)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\n",
    "\n",
    "tfidf = vectorizer.fit_transform(org_name_clean)\n",
    "print('Vecorizing completed...')\n",
    "\n",
    "# Unsupervised learner for implementing neighbor searches.\n",
    "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
    "unique_org = (df['test'].values)\n",
    "\n",
    "# distances, indices\n",
    "distances, indices = getNearestN(vectorizer, nbrs, unique_org)\n",
    "unique_org = list(unique_org)\n",
    "\n",
    "matches = []\n",
    "\n",
    "for i, j in enumerate(indices):\n",
    "    dist = round(distances[i][0], 2)\n",
    "\n",
    "    temp = [dist]\n",
    "    matches.append(temp)\n",
    "    \n",
    "matches = pd.DataFrame(matches, columns=['Match confidence'])\n",
    "df['match'] = matches['Match confidence']\n",
    "df1 = df.sort_values('match')\n",
    "df2 = df1[['Position', 'Company', 'Location']].head(10).reset_index()\n",
    "\n",
    "print(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de1508493c2f6b58da7675d475ff7fded386c0f44d06a861b289a65007a6b526"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
